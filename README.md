# ğŸ¬ Proyecto de FinalizaciÃ³n de curso Introduccion a la Ingenieria de Datos

Este proyecto implementa un pipeline completo de MLOps para construir un sistema de recomendaciÃ³n de pelÃ­culas. El flujo de trabajo integra mÃºltiples tecnologÃ­as:
- **Airbyte** para ingestiÃ³n de datos
- **DBT** para transformaciÃ³n
- **Dagster** para orquestaciÃ³n
- **MLflow** para experimentaciÃ³n y seguimiento de modelos

## ğŸ“‹ Tabla de Contenidos
- [TecnologÃ­as Utilizadas](#-tecnologÃ­as-utilizadas)
- [Arquitectura](#-arquitectura)
  - [1. Instalar Conda](#1-instalar-conda)
  - [2. Instalar Docker Desktop](#2-instalar-docker-desktop)
  - [3. Instalar y Configurar Airbyte](#3-instalar-y-configurar-airbyte)
  - [4. Configurar Conexiones en Airbyte](#4-configurar-conexiones-en-airbyte)
  - [5. Levantar Docker Compose](#5-levantar-docker-compose)
  - [6. Instalar y Configurar DBT](#6-instalar-y-configurar-dbt)
  - [7. Integrar con Dagster](#7-integrar-con-dagster)
  - [8. Configurar MLflow](#8-configurar-mlflow)
- [EjecuciÃ³n del Pipeline](#-ejecuciÃ³n-del-pipeline)
- [Estructura del Proyecto](#-estructura-del-proyecto)
- [Problemas](#-Problemas-que-tuve)

## ğŸ›  TecnologÃ­as Utilizadas

| TecnologÃ­a | FunciÃ³n |
|------------|---------|
| **Python + Conda** | GestiÃ³n de entorno y dependencias |
| **Docker + Docker Compose** | ContainerizaciÃ³n y orquestaciÃ³n de servicios |
| **Airbyte** | ExtracciÃ³n y carga de datos (EL) |
| **DBT** | TransformaciÃ³n de datos (T) |
| **Dagster** | OrquestaciÃ³n del pipeline |
| **MLflow** | Seguimiento de experimentos ML |
| **PostgreSQL** | Base de datos |

## ğŸ— Arquitectura

```
Fuente de Datos           Pipeline MLOps                Resultados
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  movies.csv   â”‚         â”‚                        â”‚    â”‚               â”‚
â”‚  critic_      â”‚â”€ â”€ â”€ â”€ â–¶â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚    â”‚   Sistema de  â”‚
â”‚  reviews.csv  â”‚         â”‚      â”‚ Airbyte â”‚      â”‚    â”‚ RecomendaciÃ³n â”‚
â”‚  user_        â”‚         â”‚      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â”‚    â”‚  de PelÃ­culas â”‚
â”‚  reviews.csv  â”‚         â”‚           â”‚           â”‚    â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”      â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚      â”‚   DBT   â”‚      â”‚            â”‚
                          â”‚      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â”‚            â”‚
                          â”‚           â”‚           â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”      â”‚    â”‚               â”‚
                          â”‚      â”‚ Dagster â”‚      â”‚    â”‚    MLflow     â”‚
                          â”‚      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â”‚    â”‚  Dashboard    â”‚
                          â”‚           â”‚           â”‚    â”‚               â”‚
                          â”‚      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”      â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚      â”‚ MLflow  â”‚      â”‚
                          â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                          â”‚                        â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ InstalaciÃ³n Paso a Paso

### 1. Instalar Conda

<img src="anaconda.png" alt="Anaconda Web" width="600">

Conda se utiliza para gestionar el entorno de Python y las dependencias:

#### Windows:
```
# 1. Descargar el instalador
   - https://www.anaconda.com/download
   - Se coloca el correo para recibir el link al instalador.

# 2. Ejecutar el instalador (GUI)
# - Seleccionar "Install for all users"
# - Mantener la ruta de instalaciÃ³n predeterminada
# - Marcar la opciÃ³n "Add Miniconda to my PATH environment variable"
# - Clic en "Install"

# 3. Verificar la instalaciÃ³n (en Windows PowerShell)
      conda --version
```

### 2. Instalar Docker Desktop

Docker y Docker Compose son necesarios para ejecutar los contenedores:

#### Windows:
1. Descargar Docker Desktop desde [docker.com](https://www.docker.com/products/docker-desktop)
2. Ejecutar el instalador y seguir las instrucciones
3. Asegurarse de que WSL 2 estÃ© habilitado (se solicitarÃ¡ durante la instalaciÃ³n) 
4. Reiniciar el sistema despuÃ©s de la instalaciÃ³n
5. Verificar la instalaciÃ³n:
```bash
docker --version
docker-compose --version
```

### 3. Instalar y Configurar Airbyte

Airbyte se utiliza para la extracciÃ³n y carga de datos:

```bash
# 1. Descargar la Ãºltima versiÃ³n de abctl. desde: https://github.com/airbytehq/abctl/releases/tag/v0.24.0
# 2. Extraer el archivo zip a la ubicaciÃ³n que prefiera. Esto crearÃ¡ una carpeta con el ejecutable abctl y otros archivos necesarios.
# 3. Copie la ruta del archivo, y agregarla en las variables de entorno.
# 4. Verifique que abctl estÃ© instalado correctamente.
      abctl version
# 5. Ejecutar Docker
# 6. Instalar Airbyte.
      abctl local install
# 7. Obtener la contraseÃ±a predeterminada.
      abctl local credentials
# 8. Ir a localhost:8000 y colocar dicha contraseÃ±a para ingresar.
```

### 4. Configurar Conexiones en Airbyte

Configurar las fuentes de datos y destinos en Airbyte:

#### AÃ±adir origen de datos (Source):
1. En la interfaz de Airbyte, seleccionar "Sources" > "New source"
2. Seleccionar "File" como tipo de origen
3. Configurar:
   - Nombre: "Movies Dataset"
   - URL: AÃ±adir la URL de los archivos desde el repositorio:
     - `https://github.com/dodobeatle/dataeng-datos/raw/main/movies.csv`
     - `https://github.com/dodobeatle/dataeng-datos/raw/main/critic_reviews.csv`
     - `https://www.dropbox.com/scl/fi/9vtdm3nvim0kepi00qlw6/user_reviews.csv?rlkey=97v0g535pjgdjxez72ea2hcvy&st=soqqnor8&dl=0`
   - Formato: CSV
   - ConfiguraciÃ³n del parser: Dejar los valores predeterminados
4. Hacer clic en "Set up source"

#### AÃ±adir destino (Destination):
1. Seleccionar "Destinations" > "New destination"
2. Seleccionar "PostgreSQL" como tipo de destino
3. Configurar:
   - Nombre: "PostgreSQL"
   - Host: `192.168.1.3`
   - Puerto: 5432
   - Usuario: beelbonacossa@gmail.com
   - Base de datos: mlops
   - Schema: target
4. Hacer clic en "Test and Save"

#### Crear conexiÃ³n:
1. Ir a "Connections" > "New connection"
2. Seleccionar:
   - Source: "movies.csv", "critic_reviews.csv", "user_reviews.csv"
   - Destination: "PostgreSQL"
3. Configurar:
   - Frecuencia de sincronizaciÃ³n: 24hs
   - Modo de destino: Raw data (no deduplication)
4. Hacer clic en "Set up connection"
5. Ejecutar la sincronizaciÃ³n manualmente haciendo clic en "Sync now"

### 5. Levantar Docker Compose

Para iniciar los servicios adicionales del proyecto:

```bash
# 1. Navegar al directorio example_compose_2
cd example_compose_2

# 2. Levantar los servicios con Docker Compose
docker-compose up -d

# 3. Verificar que todos los contenedores estÃ©n funcionando
docker ps

# Esto iniciarÃ¡:
# - nginx
# - postgres_db
# - pg_admin
# - mysql_db
```

### 6. Instalar y Configurar DBT

DBT (Data Build Tool) se utiliza para transformar los datos:

```bash
# 1. Crear y activar un entorno Conda especÃ­fico para DBT
conda create --name airbyte_env python=3.9 numpy scikit-learn pandas seaborn matplotlib
conda activate airbyte_env

# 2. Instalar DBT y luego con el adaptador PostgreSQL
pip install dbt-core
pip install dbt-postgres

# 3. Inicializar un proyecto DBT (si no existe)
dbt init dbt_testing

# 4. Configurar el perfil de conexiÃ³n
# Editar ~/.dbt/profiles.yml
# 5. Probar la conexiÃ³n
dbt debug

# 6. Crear modelos para transformar los datos
# crear los archivos dbt_testing/models/
  - movies.sql
  - movies_users.sql
  - critic_reviews.sql
  - review_stats.sql
  - user_critic_reviews.sql
  - user_reviews.sql

# 7. Configurar correctamente el schema.yml
    version: 2
    sources:
    - name: raw_data
      description: "Raw data from the mlops project"
      database: mlops
      schema: target
      tables:
        - name: movies
        - name: user_reviews
        - name: critic_reviews
```

### 7. Integrar con Dagster

Dagster orquesta el flujo de trabajo entre Airbyte, DBT y MLflow:

```bash
# 0. Haber instalado conda install -c conda-forge dagster=1.9.1 en el airbyte_env.

# 1. Crear el proyecto.
dagster project scaffold --name dg_testing

# 2. Instalar dependencias
poetry add dagster_airbyte
pip install dagster-dbt

# 3. Configurar "__init__", "definitions", Assets y Resources.

# 4. Iniciar el servicio de Dagster (si no estÃ¡ en el Docker Compose)
dagster dev
```

### 8. Configurar MLflow

MLflow se utiliza para registrar y visualizar experimentos:

```bash
# 1. Acceder a la interfaz web de MLflow
# Navegar a: http://localhost:5000

# 2. Verificar que los experimentos se registren correctamente
# DespuÃ©s de ejecutar el pipeline de Dagster, deberÃ­as ver experimentos en la interfaz de MLflow
```

## ğŸš€ EjecuciÃ³n del Pipeline

Para ejecutar el pipeline completo:

1. Asegurarse de que todos los servicios estÃ©n en funcionamiento:
   - Airbyte (http://localhost:8000)
   - Dagster (http://localhost:3000)
   - MLflow (http://localhost:5000)
   - PostgreSQL (puerto 5432)

2. Ejecutar la sincronizaciÃ³n de datos en Airbyte:
   - Acceder a la interfaz de Airbyte
   - Ir a "Connections"
   - Seleccionar la conexiÃ³n creada
   - Hacer clic en "Sync now"

3. Ejecutar el pipeline de Dagster:
   - Acceder a la interfaz de Dagster (http://localhost:3000)
   - Ir a "Assets"
   - Hacer clic en "Materialize all"

5. Visualizar los resultados en MLflow:
   - Acceder a la interfaz de MLflow (http://localhost:5000)
   - Explorar los experimentos y modelos registrados
   - Revisar las mÃ©tricas, parÃ¡metros y artefactos

## ğŸ“‚ Estructura del Proyecto

```
dg_testing/
â”œâ”€â”€ example_compose_2/            # ConfiguraciÃ³n Docker Compose.
â”‚   â”œâ”€â”€ docker-compose.yml        # Servicios para MLOps (Nginx, Postgres_db, Pg_admi, Mysql_db, MLflow)
â”‚   â””â”€â”€ .env                      # Credenciales de uso
â”œâ”€â”€ dbt-testing/                  # Proyecto DBT
â”‚   â”œâ”€â”€ models/                   # Modelos SQL de transformaciÃ³n
â”‚   â”œâ”€â”€ dbt_project.yml           # ConfiguraciÃ³n del proyecto DBT
â”‚
â”œâ”€â”€ definitions.py                # DefiniciÃ³n del pipeline
â”œâ”€â”€ dbt.integration.yml           # Conexion con dbt
â”‚
â”œâ”€â”€ resources/
â”‚   â””â”€â”€ __init__.py               # Conexiones con la base de datos.
â”‚
â”œâ”€â”€ Assets/                       # Los Assets necesarios para el proyecto.
â”‚   â”œâ”€â”€ airbyte.py      
â”‚   â”œâ”€â”€ dbt.py      
â”‚   â”œâ”€â”€ model_helper.py
â”‚   â””â”€â”€ train_model.py
```

## ğŸ”§ Problemas que tuve:

1. **Error con Docker**
   - Docker si bien se instalo de forma correcta, fallaba mostrando cualquier cantidad de errores, leyendos foros y viendo videos me di cuenta que necesitaba tener instalado un sub-sistema Linux en Windows para hacerlo andar: 'wsl --install'
   - InstalaciÃ³n de Airbyte: Esta instalaciÃ³n en teoria deberia demorarse 15/30 minutos, se demoro mÃ¡s de 3 horas por algun motivo, fui comprobando los logs de la instalaciÃ³n por lo que podia ver que efectivamente se estaba instalando pero de forma muy lenta. Verifique       que no era mi conexion, y mis componentes estaban dentro de los recomendados.

2. **Errores con Dagster**
   - Almacenamiento: Comence el curso con 200 GB libres, pero Airbyte, y los demas programas empezaron llenar mi disco (SSD de 480 GB), ahora mismo tengo 30 GB disponibles unicamente.
   - Errores de codificacion y tipeo, en varias ocasiones no funcionaba la conexion con Airbyte o postgress por errores mios por malos tipeos, faltaba una "S" o sobraban letras.
   - Ahora mismo no puedo correr el proyecto debido a la falta de espacio, tambien de potencia, en un testeo me fallo Dagster con un error que indicaba que los componentes no eran suficientemente potentes para la ejecucion.

2. **Errores Generales**
   - Debi haber elegido un Source con informaciÃ³n mas ligero.

---

## ğŸ”§ Conclusiones:

Aprendi mucho durante este curso, no tengo dudas que cuando se presente en un futuro la necesidad reverÃ© estas clases y repasare los apuntes que hice de las mismas. 
El profesor Pedro fue Impecable durante cada clase, espero tenerlo en alguna materia mas adelante. 

---
Proyecto Final del Taller de Introduccion a la Ingenieria de Datos.
